{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543\n",
    "def fix(env, seed):\n",
    "    env.action_space.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "import gymnasium as gym\n",
    "import random\n",
    "env = gym.make('LunarLander-v2' ,render_mode='rgb_array')\n",
    "fix(env, seed) # fix the environment Do not revise this !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.00270901,  1.421877  , -0.27441233,  0.48697796,  0.00314588,\n",
      "        0.06215852,  0.        ,  0.        ], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Memory\n",
    "\n",
    "store the actions, states, rewards, is_termminal, and log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: ActorCritic\n",
    "\n",
    "### 2 fully connected networks\n",
    "1. actor/action layer: input the states, follow the policy and yields an action\n",
    "2. critic/value layer: input the states, evaluate the current ? //TODO\n",
    "\n",
    "### act function\n",
    "\n",
    "Generate a probability distribution of the actions using action layer taking state as the input, sample one action and store these information in the memory.\n",
    "\n",
    "### evaluate function\n",
    "\n",
    "Take a state as input and generate action probability distribution, calculate the entropy of this distribution and evaluate the log probability of the given action. Use the critic to evaluate the expected rewards under current state. Return all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, n_latent_var),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "\n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "               nn.Linear(state_dim, 128),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(128, n_latent_var),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(n_latent_var, 1)\n",
    "               )\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO agent\n",
    "\n",
    "### gamma\n",
    "\n",
    "if close to 1, future rewards will be considered more\n",
    "if close to 0, current rewards wiil take more proportion\n",
    "\n",
    "### state_values\n",
    "\n",
    "the output of the critic, which evaluates the state and give an expected future reward\n",
    "\n",
    "### rewards\n",
    "\n",
    "the static list of discounted rewards calculated by the data in old trajectory (when adopting old policy)\n",
    "\n",
    "### advantages = rewards - state_values.detach()\n",
    "\n",
    "how better the old policy perform than current policy. if positive, actual reward is higher than state value, it is better to stick to current action, vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var,update_timestep, lr, betas, gamma, K_epochs, eps_clip, c1, c2):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.update_timestep = update_timestep\n",
    "        self.timestep = 0\n",
    "        self.memory = Memory()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.update_cnt = 0\n",
    "\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "\n",
    "    def update(self):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(self.memory.states).detach().to(device)\n",
    "        old_actions = torch.stack(self.memory.actions).detach().to(device)\n",
    "        old_logprobs = torch.stack(self.memory.logprobs).detach().to(device)\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values \n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss \n",
    "            advantages = rewards - state_values.detach()\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages # clip to avoid large difference between samples\n",
    "            loss = -torch.min(surr1, surr2)  + self.c1*self.MseLoss(state_values, rewards) + self.c2*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    def step(self, reward, done):\n",
    "        self.timestep += 1 \n",
    "        # Saving reward and is_terminal:\n",
    "        self.memory.rewards.append(reward)\n",
    "        self.memory.is_terminals.append(done)\n",
    "\n",
    "        # update the policy per \"update_timestep\"\n",
    "        if self.timestep % self.update_timestep == 0:\n",
    "            self.update()\n",
    "            self.memory.clear_memory()\n",
    "            self.timestep = 0\n",
    "            self.update_cnt += 1\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.policy_old.act(state, self.memory)\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "        self.policy_old.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy.eval()\n",
    "        self.policy_old.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(total_rewards):\n",
    "    if len(total_rewards) == 0:\n",
    "        return 0\n",
    "    if len(total_rewards) < 99:\n",
    "        return np.mean(total_rewards)\n",
    "    else:\n",
    "        return np.mean(total_rewards[-100:])\n",
    "    \n",
    "def reward_shaping(pre_state, state, type):\n",
    "    if (type == \"center\"):\n",
    "        pre_shaping = -20*np.abs(pre_state[0])\n",
    "        shaping = -20*np.abs(state[0])\n",
    "    if (type == \"horizontal\"):\n",
    "        pre_shaping = -1*np.abs(pre_state[4])\n",
    "        shaping = -1*np.abs(state[4])\n",
    "    if (type == \"both\"):\n",
    "        pre_shaping = -10*np.abs(pre_state[0])-5*abs(pre_state[4])\n",
    "        shaping = -10*np.abs(state[0])-5*abs(state[4])\n",
    "    if (type == \"none\"):\n",
    "        pre_shaping, shaping = 0, 0\n",
    "    return shaping - pre_shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainOnce(c2, eps_clip, agent, reward_mode = \"none\", target_score = 200, target_success_rate = 0.9, max_episodes=2000, max_steps=1500):\n",
    "    agent.train()\n",
    "    print(f\"Training with c2: {c2}, eps_clip: {eps_clip}\")\n",
    "    total_rewards = []\n",
    "    final_rewards = []\n",
    "    moving_average_rewards = []\n",
    "    num_episodes = 0\n",
    "    success_rate = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        total_shaped_reward = 0\n",
    "        step = 0\n",
    "        # update_cnt = agent.update_cnt\n",
    "        while True:\n",
    "            step +=1\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            shaped_reward = reward + reward_shaping(state, next_state, reward_mode)\n",
    "            total_shaped_reward += shaped_reward # use the shaped reward to train\n",
    "            agent.step(shaped_reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            if done or step >= max_steps:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "        moving_average_rewards.append(moving_average(total_rewards))\n",
    "        success_rate = final_rewards.count(100) / 300\n",
    "\n",
    "        if (num_episodes % 100 == 0):\n",
    "            print(f\"Update Count: {agent.update_cnt}, average rewards: {moving_average_rewards[-1]}, episode: {num_episodes}, success rate: {success_rate}\")\n",
    "        \n",
    "        # finish the training if any one condition is reached\n",
    "        if (moving_average(total_rewards) >= target_score):\n",
    "            print(f\"Target score reached in {num_episodes} episodes!\")\n",
    "            break\n",
    "        if (success_rate >= target_success_rate):\n",
    "            print(f\"Target success rate reached in {num_episodes} episodes!\")\n",
    "            break\n",
    "        if (num_episodes >= max_episodes):\n",
    "            print(f\"Max episodes {max_episodes} reached!\")\n",
    "            break\n",
    "\n",
    "    training_result = {\n",
    "        \"total_rewards\": total_rewards,\n",
    "        \"moving_average_rewards\": moving_average_rewards,\n",
    "        \"final_rewards\": final_rewards,\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    return training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateOnce(agent, eval_num = 500):\n",
    "    agent.eval()\n",
    "    test_total_rewards = []\n",
    "    test_final_rewards = []\n",
    "    success_rate = 0\n",
    "    prg_bar = tqdm(range(eval_num))\n",
    "    for i in prg_bar:\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done :\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        test_total_rewards.append(total_reward)\n",
    "        test_final_rewards.append(reward)\n",
    "    success_rate = test_final_rewards.count(100) / eval_num\n",
    "    print(f\"Test success rate {success_rate} over {eval_num} games\")\n",
    "    eval_result = {\n",
    "        \"test_total_rewards\": test_total_rewards,\n",
    "        \"test_final_rewards\": test_final_rewards,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BayesianOptimization to find the best pair of c2 and eps_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "state_dim = 8 \n",
    "action_dim = 4 \n",
    "n_latent_var = 64\n",
    "update_timestep = 1000\n",
    "c1 = 0.5\n",
    "lr = 0.003\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99\n",
    "K_epochs = 4\n",
    "\n",
    "if (True):\n",
    "    p_bounds = {'c2': (-0.01, -0.005), 'eps_clip': (0.1,0.3)}\n",
    "\n",
    "    def PPO_fn(c2, eps_clip):\n",
    "        agent = PPOAgent(state_dim,action_dim,n_latent_var,update_timestep,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "        training_result = TrainOnce(c2, eps_clip, agent, target_score=100)\n",
    "        return -training_result[\"num_episodes\"]\n",
    "    HyperParamOptimizer = BayesianOptimization(\n",
    "        f=PPO_fn,\n",
    "        pbounds=p_bounds,\n",
    "        random_state=1\n",
    "    )\n",
    "    HyperParamOptimizer.probe(\n",
    "        params=[-0.006, 0.22],\n",
    "        lazy=True\n",
    "    )\n",
    "    HyperParamOptimizer.maximize(\n",
    "        init_points=2,\n",
    "        n_iter=7\n",
    "    )\n",
    "    print(HyperParamOptimizer.max)\n",
    "    c2 = HyperParamOptimizer.max['params']['c2']\n",
    "    eps_clip = HyperParamOptimizer.max['params']['eps_clip']\n",
    "else:\n",
    "    c2 = -0.005861493516941971\n",
    "    eps_clip = 0.22143098426065316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a excellent model\n",
    "\n",
    "## Keep training until landing success rate over previous 300 episodes reaches 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "excellent_result = TrainOnce(c2, eps_clip, agent, target_score=300, target_success_rate=0.95, max_episodes=5000, max_steps=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_old.state_dict(), f'PPO_LunarLander_{N}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'PPO_LunarLander_{N}.pth')\n",
    "model = PPOAgent(state_dim ,action_dim,n_latent_var,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model.policy_old.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = EvaluateOnce(model, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the evaluation result and training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "total_rewards = excellent_result['total_rewards']\n",
    "final_rewards = excellent_result['final_rewards']\n",
    "moving_average_rewards = excellent_result['moving_average_rewards']\n",
    "test_total_rewards = eval_result['test_total_rewards']\n",
    "test_final_rewards = eval_result['test_final_rewards']\n",
    "success_rate = eval_result['success_rate']\n",
    "data = {'total_rewards': total_rewards, 'final_rewards': final_rewards, 'moving_average_rewards': moving_average_rewards, 'test_total_rewards': test_total_rewards, 'test_final_rewards': test_final_rewards, 'success_rate': success_rate}\n",
    "\n",
    "with open(f'PPO_Result_{N}.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Saving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_demo(agent, num=10, mode = 'trail'):\n",
    "    agent.eval()\n",
    "    fig = plt.figure()\n",
    "    total_rewards = []\n",
    "\n",
    "    for i in range(num):\n",
    "        ims = []\n",
    "        agent.eval()\n",
    "        actions = []\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done :\n",
    "            step +=1\n",
    "            action = model.act(state)\n",
    "            actions.append(action)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            im = plt.imshow(env.render(), animated=True)\n",
    "            ims.append([im])\n",
    "        total_rewards.append(total_reward)\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=25, blit=True, repeat_delay=1000)\n",
    "        if mode=='trail':\n",
    "            ani.save(f'./demo_gifs/trial{N}/demo_{N}_{i}.gif')\n",
    "        if mode=='reward shaping':\n",
    "            ani.save(f'./demo_gifs/reward_shaping{N}/reward_shaping_{N}_{i}.gif')\n",
    "        if mode=='compare':\n",
    "            ani.save(f'./demo_gifs/reward_shaping{N}/compare_{N}_{i}.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 10 demos for model loaded just now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_demo(model, 10, 'trail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix(env, 666)\n",
    "r_s_agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "training_result_rs = TrainOnce(c2, eps_clip, r_s_agent, reward_mode=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_demo(r_s_agent, 5, 'reward shaping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to normal process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix(env, 666)\n",
    "normal_agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "training_result_normal = TrainOnce(c2, eps_clip, normal_agent, reward_mode=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_demo(normal_agent, 5, 'compare')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
