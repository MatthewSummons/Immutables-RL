{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543\n",
    "def fix(env, seed):\n",
    "    env.action_space.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "import gymnasium as gym\n",
    "import random\n",
    "env = gym.make('LunarLander-v2' ,render_mode='rgb_array')\n",
    "fix(env, seed) # fix the environment Do not revise this !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.00342045,  1.4145806 ,  0.34643883,  0.16267964, -0.00395666,\n",
      "       -0.07847356,  0.        ,  0.        ], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Memory\n",
    "\n",
    "store the actions, states, rewards, is_termminal, and log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: ActorCritic\n",
    "\n",
    "### 2 fully connected networks\n",
    "1. actor/action layer: input the states, follow the policy and yields an action\n",
    "2. critic/value layer: input the states, evaluate the current ? //TODO\n",
    "\n",
    "### act function\n",
    "\n",
    "Generate a probability distribution of the actions using action layer taking state as the input, sample one action and store these information in the memory.\n",
    "\n",
    "### evaluate function\n",
    "\n",
    "Take a state as input and generate action probability distribution, calculate the entropy of this distribution and evaluate the log probability of the given action. Use the critic to evaluate the expected rewards under current state. Return all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, n_latent_var),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "\n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "               nn.Linear(state_dim, 128),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(128, n_latent_var),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(n_latent_var, 1)\n",
    "               )\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO agent\n",
    "\n",
    "### gamma\n",
    "\n",
    "if close to 1, future rewards will be considered more\n",
    "if close to 0, current rewards wiil take more proportion\n",
    "\n",
    "### state_values\n",
    "\n",
    "the output of the critic, which evaluates the state and give an expected future reward\n",
    "\n",
    "### rewards\n",
    "\n",
    "the static list of discounted rewards calculated by the data in old trajectory (when adopting old policy)\n",
    "\n",
    "### advantages = rewards - state_values.detach()\n",
    "\n",
    "how better the old policy perform than current policy. if positive, actual reward is higher than state value, it is better to stick to current action, vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var,update_timestep, lr, betas, gamma, K_epochs, eps_clip, c1, c2):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.update_timestep = update_timestep\n",
    "        self.timestep = 0\n",
    "        self.memory = Memory()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.update_cnt = 0\n",
    "\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "\n",
    "    def update(self):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(self.memory.states).detach().to(device)\n",
    "        old_actions = torch.stack(self.memory.actions).detach().to(device)\n",
    "        old_logprobs = torch.stack(self.memory.logprobs).detach().to(device)\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values \n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss \n",
    "            advantages = rewards - state_values.detach()\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages # clip to avoid large difference between samples\n",
    "            loss = -torch.min(surr1, surr2)  + self.c1*self.MseLoss(state_values, rewards) + self.c2*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    def step(self, reward, done):\n",
    "        self.timestep += 1 \n",
    "        # Saving reward and is_terminal:\n",
    "        self.memory.rewards.append(reward)\n",
    "        self.memory.is_terminals.append(done)\n",
    "\n",
    "        # update the policy per \"update_timestep\"\n",
    "        if self.timestep % self.update_timestep == 0:\n",
    "            self.update()\n",
    "            self.memory.clear_memory()\n",
    "            self.timestep = 0\n",
    "            self.update_cnt += 1\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.policy_old.act(state, self.memory)\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "        self.policy_old.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy.eval()\n",
    "        self.policy_old.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(total_rewards):\n",
    "    if len(total_rewards) == 0:\n",
    "        return 0\n",
    "    if len(total_rewards) < 99:\n",
    "        return np.mean(total_rewards)\n",
    "    else:\n",
    "        return np.mean(total_rewards[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Function for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainOnce(c2, eps_clip, agent):\n",
    "    print(f\"Training with c2: {c2}, eps_clip: {eps_clip}\")\n",
    "    total_rewards = []\n",
    "    final_rewards = []\n",
    "    moving_average_rewards = []\n",
    "    num_episodes = 0\n",
    "    while(moving_average(total_rewards)<200):\n",
    "        num_episodes += 1\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        update_cnt = agent.update_cnt\n",
    "        # collect trajectory\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            agent.step(reward, done)\n",
    "            if done:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "        moving_average_rewards.append(moving_average(total_rewards))\n",
    "        if (update_cnt != agent.update_cnt):\n",
    "            print(f\"Update Count: {agent.update_cnt}, average rewards: {moving_average(total_rewards)}, episode: {num_episodes}\")\n",
    "        \n",
    "    print(f\"Training finished with {num_episodes} episodes\")\n",
    "    return -num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BayesianOptimization to find the best pair of c2 and eps_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    c2     | eps_clip  |\n",
      "-------------------------------------------------\n",
      "Training with c2: -0.006, eps_clip: 0.22\n",
      "Update Count: 1, average rewards: -192.57007750642316, episode: 11\n",
      "Update Count: 2, average rewards: -160.50157114113594, episode: 24\n",
      "Update Count: 3, average rewards: -191.68583283190193, episode: 36\n",
      "Update Count: 4, average rewards: -180.9306206687213, episode: 48\n",
      "Update Count: 5, average rewards: -168.16701282141707, episode: 61\n",
      "Update Count: 6, average rewards: -155.7310982192528, episode: 74\n",
      "Update Count: 7, average rewards: -152.17751001914118, episode: 88\n",
      "Update Count: 8, average rewards: -152.4488462431514, episode: 101\n",
      "Update Count: 9, average rewards: -147.9803831150762, episode: 114\n",
      "Update Count: 10, average rewards: -142.99807628151132, episode: 127\n",
      "Update Count: 11, average rewards: -132.37885492714898, episode: 140\n",
      "Update Count: 12, average rewards: -137.17446962947943, episode: 152\n",
      "Update Count: 13, average rewards: -140.1923964259969, episode: 165\n",
      "Update Count: 14, average rewards: -140.43773628816254, episode: 177\n",
      "Update Count: 15, average rewards: -137.38384355755866, episode: 189\n",
      "Update Count: 16, average rewards: -128.7289823835119, episode: 201\n",
      "Update Count: 17, average rewards: -120.446225236643, episode: 213\n",
      "Update Count: 18, average rewards: -116.28911322125383, episode: 226\n",
      "Update Count: 19, average rewards: -107.34021989263707, episode: 238\n",
      "Update Count: 20, average rewards: -105.49357842914753, episode: 247\n",
      "Update Count: 21, average rewards: -96.45346391895845, episode: 256\n",
      "Update Count: 22, average rewards: -88.47666848166405, episode: 266\n",
      "Update Count: 23, average rewards: -79.08823846819773, episode: 277\n",
      "Update Count: 24, average rewards: -72.6722801369728, episode: 286\n",
      "Update Count: 25, average rewards: -70.63559205749495, episode: 295\n",
      "Update Count: 26, average rewards: -66.42229200056903, episode: 306\n",
      "Update Count: 27, average rewards: -63.25956320498651, episode: 312\n",
      "Update Count: 28, average rewards: -60.830772738401386, episode: 316\n",
      "Update Count: 29, average rewards: -63.50694392648253, episode: 325\n",
      "Update Count: 30, average rewards: -60.072255303442056, episode: 333\n",
      "Update Count: 31, average rewards: -66.44908003063654, episode: 339\n",
      "Update Count: 32, average rewards: -57.478398166395415, episode: 341\n",
      "Update Count: 33, average rewards: -52.566933323983136, episode: 347\n",
      "Update Count: 34, average rewards: -64.58503495064541, episode: 356\n",
      "Update Count: 35, average rewards: -71.27286550324516, episode: 365\n",
      "Update Count: 36, average rewards: -76.64947064703583, episode: 371\n",
      "Update Count: 37, average rewards: -81.3047605725463, episode: 378\n",
      "Update Count: 38, average rewards: -87.13147224500445, episode: 385\n",
      "Update Count: 39, average rewards: -89.24001828708029, episode: 391\n",
      "Update Count: 40, average rewards: -88.8179584991467, episode: 396\n",
      "Update Count: 41, average rewards: -91.33292535577299, episode: 401\n",
      "Update Count: 42, average rewards: -94.98776926775926, episode: 408\n",
      "Update Count: 43, average rewards: -89.17420763159913, episode: 410\n",
      "Update Count: 44, average rewards: -88.56262513272019, episode: 415\n",
      "Update Count: 45, average rewards: -83.92442021147309, episode: 419\n",
      "Update Count: 46, average rewards: -79.25276678633553, episode: 424\n",
      "Update Count: 47, average rewards: -77.3931806694775, episode: 429\n",
      "Update Count: 48, average rewards: -71.80083564269859, episode: 432\n",
      "Update Count: 49, average rewards: -72.05127710144387, episode: 436\n",
      "Update Count: 50, average rewards: -68.07934424627295, episode: 440\n",
      "Update Count: 51, average rewards: -71.26614423756058, episode: 443\n",
      "Update Count: 52, average rewards: -69.49013457971259, episode: 447\n",
      "Update Count: 53, average rewards: -67.76469908015255, episode: 453\n",
      "Update Count: 54, average rewards: -61.26666088556905, episode: 458\n",
      "Update Count: 55, average rewards: -47.107662557253065, episode: 462\n",
      "Update Count: 56, average rewards: -42.83464087384673, episode: 465\n",
      "Update Count: 57, average rewards: -37.72915704490952, episode: 468\n",
      "Update Count: 58, average rewards: -28.840564277230524, episode: 472\n",
      "Update Count: 59, average rewards: -15.371448002239827, episode: 476\n",
      "Update Count: 60, average rewards: -3.6312129764227756, episode: 480\n",
      "Update Count: 61, average rewards: 5.811889053107291, episode: 487\n",
      "Update Count: 62, average rewards: 18.99303428175266, episode: 491\n",
      "Update Count: 63, average rewards: 22.119152608152227, episode: 493\n",
      "Update Count: 64, average rewards: 28.013516219482927, episode: 495\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "state_dim = 8 \n",
    "action_dim = 4 \n",
    "n_latent_var = 64\n",
    "update_timestep = 1000\n",
    "c1 = 0.5\n",
    "lr = 0.003                 \n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99\n",
    "K_epochs = 4\n",
    "\n",
    "if (True):\n",
    "    p_bounds = {'c2': (-0.01, -0.005), 'eps_clip': (0.1,0.3)}\n",
    "\n",
    "    def PPO_fn(c2, eps_clip):\n",
    "        agent = PPOAgent(state_dim,action_dim,n_latent_var,update_timestep,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "        agent.train()\n",
    "        return TrainOnce(c2, eps_clip, agent)\n",
    "    HyperParamOptimizer = BayesianOptimization(\n",
    "        f=PPO_fn,\n",
    "        pbounds=p_bounds,\n",
    "        random_state=1\n",
    "    )\n",
    "    HyperParamOptimizer.probe(\n",
    "        params=[-0.006, 0.22],\n",
    "        lazy=True\n",
    "    )\n",
    "    HyperParamOptimizer.maximize(\n",
    "        init_points=2,\n",
    "        n_iter=10\n",
    "    )\n",
    "    print(HyperParamOptimizer.max)\n",
    "    c2 = HyperParamOptimizer.max['params']['c2']\n",
    "    eps_clip = HyperParamOptimizer.max['params']['eps_clip']\n",
    "else:\n",
    "    c2 = -0.005861493516941971\n",
    "    eps_clip = 0.22143098426065316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a excellent model\n",
    "\n",
    "## Keep training until landing success rate over previous 300 episodes reaches 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "total_rewards = []\n",
    "final_rewards = []\n",
    "moving_average_rewards = []\n",
    "num_episodes = 0\n",
    "test_number = 300\n",
    "success_rate = 0\n",
    "agent.train()\n",
    "while True:\n",
    "    num_episodes += 1\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    update_cnt = agent.update_cnt\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        agent.step(reward, done)\n",
    "        if done:\n",
    "            final_rewards.append(reward)\n",
    "            total_rewards.append(total_reward)\n",
    "            break\n",
    "    moving_average_rewards.append(moving_average(total_rewards))\n",
    "    success_rate = final_rewards[-test_number:].count(100) / test_number\n",
    "\n",
    "    if (update_cnt != agent.update_cnt):\n",
    "        print(f\"Update Count: {agent.update_cnt}, average rewards: {moving_average(total_rewards)}, episodes: {num_episodes}, success rate: {success_rate}\")\n",
    "\n",
    "    # if (moving_average(total_rewards) > 280 or num_episodes > 5000 or success_rate > 0.95):\n",
    "    if (success_rate > 0.95):\n",
    "        break\n",
    "\n",
    "print(f\"Training finished with {num_episodes} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_old.state_dict(), f'PPO_LunarLander_{N}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'PPO_LunarLander_{N}.pth')\n",
    "model = PPOAgent(state_dim ,action_dim,n_latent_var,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model.policy_old.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_total_rewards = []\n",
    "test_final_rewards = []\n",
    "step_used_list = []\n",
    "success_rate = 0\n",
    "prg_bar = tqdm(range(500))\n",
    "for i in prg_bar:\n",
    "    actions = []\n",
    "    state = env.reset()[0]\n",
    "    # img = plt.imshow(env.render())\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done :\n",
    "        step +=1\n",
    "        action = model.act(state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    test_total_rewards.append(total_reward)\n",
    "    test_final_rewards.append(reward)\n",
    "    step_used_list.append(step)\n",
    "success_rate = test_final_rewards.count(100) / 500\n",
    "print(f\"Success rate: {success_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the evaluation result and training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {'total_rewards': total_rewards, 'final_rewards': final_rewards, 'moving_average_rewards': moving_average_rewards, 'test_total_rewards': test_total_rewards, 'test_final_rewards': test_final_rewards, 'step_used_list': step_used_list}\n",
    "\n",
    "with open(f'PPO_Result_{N}.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "save 10 gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fix(env, seed)\n",
    "fig = plt.figure()\n",
    "total_rewards = []\n",
    "animations = []\n",
    "\n",
    "for i in range(10):\n",
    "    ims = []\n",
    "    model.eval()\n",
    "    actions = []\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done :\n",
    "        step +=1\n",
    "        action = model.act(state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        im = plt.imshow(env.render(), animated=True)\n",
    "        ims.append([im])\n",
    "    total_rewards.append(total_reward)\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=25, blit=True, repeat_delay=1000)\n",
    "    ani.save(f'./demo_gifs/trial{N}/demo_{N}_{i}.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
