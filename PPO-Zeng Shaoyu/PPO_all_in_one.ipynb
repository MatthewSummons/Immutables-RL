{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543\n",
    "def fix(env, seed):\n",
    "    env.action_space.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "import gymnasium as gym\n",
    "import random\n",
    "env = gym.make('LunarLander-v2' ,render_mode='rgb_array')\n",
    "fix(env, seed) # fix the environment Do not revise this !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00182114,  1.4182885 , -0.18447421,  0.3274784 ,  0.00211701,\n",
       "         0.04178616,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you don't want to overwrite your previous results, increment the N by 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Memory\n",
    "\n",
    "store the actions, states, rewards, is_termminal, and log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: ActorCritic\n",
    "\n",
    "### 2 fully connected networks\n",
    "1. actor/action layer: input the states, follow the policy and yields an action\n",
    "2. critic/value layer: input the states, evaluate the current estimated rewards\n",
    "\n",
    "### act function\n",
    "\n",
    "Generate a probability distribution of the actions using action layer taking state as the input, sample one action and store these information in the memory.\n",
    "\n",
    "### evaluate function\n",
    "\n",
    "Take a state as input and generate action probability distribution, calculate the entropy of this distribution and evaluate the log probability of the given action. Use the critic to evaluate the expected rewards under current state. Return all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, n_latent_var),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "\n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "               nn.Linear(state_dim, 128),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(128, n_latent_var),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(n_latent_var, 1)\n",
    "               )\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO agent\n",
    "\n",
    "### gamma\n",
    "\n",
    "if close to 1, future rewards will be considered more\n",
    "if close to 0, current rewards wiil take more proportion\n",
    "\n",
    "### state_values\n",
    "\n",
    "the output of the critic, which evaluates the state and give an expected future reward\n",
    "\n",
    "### rewards\n",
    "\n",
    "the static list of discounted rewards calculated by the data in old trajectory (when adopting old policy)\n",
    "\n",
    "### advantages = rewards - state_values.detach()\n",
    "\n",
    "how better the old policy perform than current policy. if positive, actual reward is higher than state value, it is better to stick to current action, vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var,update_timestep, lr, betas, gamma, K_epochs, eps_clip, c1, c2):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.update_timestep = update_timestep\n",
    "        self.timestep = 0\n",
    "        self.memory = Memory()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.update_cnt = 0\n",
    "\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "\n",
    "    def update(self):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(self.memory.states).detach().to(device)\n",
    "        old_actions = torch.stack(self.memory.actions).detach().to(device)\n",
    "        old_logprobs = torch.stack(self.memory.logprobs).detach().to(device)\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values \n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss \n",
    "            advantages = rewards - state_values.detach()\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages # clip to avoid large difference between samples\n",
    "            loss = -torch.min(surr1, surr2)  + self.c1*self.MseLoss(state_values, rewards) + self.c2*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    def step(self, reward, done):\n",
    "        self.timestep += 1 \n",
    "        # Saving reward and is_terminal:\n",
    "        self.memory.rewards.append(reward)\n",
    "        self.memory.is_terminals.append(done)\n",
    "\n",
    "        # update the policy per \"update_timestep\"\n",
    "        if self.timestep % self.update_timestep == 0:\n",
    "            self.update()\n",
    "            self.memory.clear_memory()\n",
    "            self.timestep = 0\n",
    "            self.update_cnt += 1\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.policy_old.act(state, self.memory)\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "        self.policy_old.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy.eval()\n",
    "        self.policy_old.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(total_rewards):\n",
    "    if len(total_rewards) == 0:\n",
    "        return 0\n",
    "    if len(total_rewards) < 99:\n",
    "        return np.mean(total_rewards)\n",
    "    else:\n",
    "        return np.mean(total_rewards[-100:])\n",
    "    \n",
    "def reward_shaping(pre_state, state, type):\n",
    "    if (type == \"center\"):\n",
    "        pre_shaping = -20*np.abs(pre_state[0])\n",
    "        shaping = -20*np.abs(state[0])\n",
    "    if (type == \"horizontal\"):\n",
    "        pre_shaping = -1*np.abs(pre_state[4])\n",
    "        shaping = -1*np.abs(state[4])\n",
    "    if (type == \"both\"):\n",
    "        pre_shaping = -10*np.abs(pre_state[0])-5*abs(pre_state[4])\n",
    "        shaping = -10*np.abs(state[0])-5*abs(state[4])\n",
    "    if (type == \"none\"):\n",
    "        pre_shaping, shaping = 0, 0\n",
    "    return shaping - pre_shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainOnce(c2, eps_clip, agent, reward_mode = \"none\", target_score = 200, target_success_rate = 0.9, max_episodes=2000, max_steps=900):\n",
    "    agent.train()\n",
    "    print(f\"Training with c2: {c2}, eps_clip: {eps_clip}\")\n",
    "    total_rewards = []\n",
    "    final_rewards = []\n",
    "    moving_average_rewards = []\n",
    "    num_episodes = 0\n",
    "    success_rate = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        total_shaped_reward = 0\n",
    "        step = 0\n",
    "        # update_cnt = agent.update_cnt\n",
    "        while True:\n",
    "            step +=1\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            shaped_reward = reward + reward_shaping(state, next_state, reward_mode)\n",
    "            total_shaped_reward += shaped_reward # use the shaped reward to train\n",
    "            agent.step(shaped_reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            # if done or step >= max_steps:\n",
    "            if done:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "        moving_average_rewards.append(moving_average(total_rewards))\n",
    "        success_rate = final_rewards[-300:].count(100) / 300\n",
    "\n",
    "        if (num_episodes % 100 == 0):\n",
    "            print(f\"Update Count: {agent.update_cnt}, average rewards: {moving_average_rewards[-1]}, episode: {num_episodes}, success rate: {success_rate}\")\n",
    "        \n",
    "        # finish the training if any one condition is reached\n",
    "        if (moving_average(total_rewards) >= target_score):\n",
    "            print(f\"Target score reached in {num_episodes} episodes!\")\n",
    "            break\n",
    "        if (success_rate >= target_success_rate):\n",
    "            print(f\"Target success rate reached in {num_episodes} episodes!\")\n",
    "            break\n",
    "        if (num_episodes >= max_episodes):\n",
    "            print(f\"Max episodes {max_episodes} reached!\")\n",
    "            break\n",
    "\n",
    "    training_result = {\n",
    "        \"total_rewards\": total_rewards,\n",
    "        \"moving_average_rewards\": moving_average_rewards,\n",
    "        \"final_rewards\": final_rewards,\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    return training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateOnce(agent, eval_num = 500):\n",
    "    agent.eval()\n",
    "    test_total_rewards = []\n",
    "    test_final_rewards = []\n",
    "    success_rate = 0\n",
    "    prg_bar = tqdm(range(eval_num))\n",
    "    for i in prg_bar:\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "            if (step>3000):\n",
    "                done = True\n",
    "        test_total_rewards.append(total_reward)\n",
    "        test_final_rewards.append(reward)\n",
    "    success_rate = test_final_rewards.count(100) / eval_num\n",
    "    print(f\"Test success rate {success_rate} over {eval_num} games\")\n",
    "    eval_result = {\n",
    "        \"test_total_rewards\": test_total_rewards,\n",
    "        \"test_final_rewards\": test_final_rewards,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BayesianOptimization to find the best pair of c2 and eps_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    c2     | eps_clip  |\n",
      "-------------------------------------------------\n",
      "Training with c2: -0.006, eps_clip: 0.22\n",
      "Update Count: 8, average rewards: -158.87821519972252, episode: 100, success rate: 0.0\n",
      "Update Count: 16, average rewards: -123.11784075628591, episode: 200, success rate: 0.0\n",
      "Update Count: 26, average rewards: -102.3209353330822, episode: 300, success rate: 0.0\n",
      "Update Count: 56, average rewards: -79.54429496982868, episode: 400, success rate: 0.0033333333333333335\n",
      "Update Count: 69, average rewards: -2.081509871049998, episode: 500, success rate: 0.006666666666666667\n",
      "Update Count: 98, average rewards: 52.51964714999291, episode: 600, success rate: 0.11333333333333333\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "state_dim = 8 \n",
    "action_dim = 4 \n",
    "n_latent_var = 64\n",
    "update_timestep = 1000\n",
    "c1 = 0.5\n",
    "lr = 0.003\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99\n",
    "K_epochs = 4\n",
    "\n",
    "if (True):\n",
    "    p_bounds = {'c2': (-0.01, -0.005), 'eps_clip': (0.1,0.3)}\n",
    "\n",
    "    def PPO_fn(c2, eps_clip):\n",
    "        agent = PPOAgent(state_dim,action_dim,n_latent_var,update_timestep,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "        training_result = TrainOnce(c2, eps_clip, agent, target_score=200)\n",
    "        return -training_result[\"num_episodes\"]\n",
    "    HyperParamOptimizer = BayesianOptimization(\n",
    "        f=PPO_fn,\n",
    "        pbounds=p_bounds,\n",
    "        random_state=1\n",
    "    )\n",
    "    HyperParamOptimizer.probe(\n",
    "        params=[-0.006, 0.22],\n",
    "        lazy=True\n",
    "    )\n",
    "    HyperParamOptimizer.maximize(\n",
    "        init_points=2,\n",
    "        n_iter=7\n",
    "    )\n",
    "    print(HyperParamOptimizer.max)\n",
    "    c2 = HyperParamOptimizer.max['params']['c2']\n",
    "    eps_clip = HyperParamOptimizer.max['params']['eps_clip']\n",
    "else:\n",
    "    c2=-0.006\n",
    "    eps_clip=0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a excellent model\n",
    "\n",
    "## Keep training until landing success rate over previous 300 episodes reaches 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with c2: -0.006, eps_clip: 0.22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n\u001b[0;32m----> 2\u001b[0m excellent_result \u001b[38;5;241m=\u001b[39m \u001b[43mTrainOnce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m270\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_success_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 19\u001b[0m, in \u001b[0;36mTrainOnce\u001b[0;34m(c2, eps_clip, agent, reward_mode, target_score, target_success_rate, max_episodes, max_steps)\u001b[0m\n\u001b[1;32m     17\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m---> 19\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     22\u001b[0m shaped_reward \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m reward_shaping(state, next_state, reward_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo_zsy/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo_zsy/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo_zsy/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo_zsy/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:631\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    617\u001b[0m         p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    618\u001b[0m             (\n\u001b[1;32m    619\u001b[0m                 ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    626\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    627\u001b[0m         impulse_pos,\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[0;32m--> 631\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    634\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "excellent_result = TrainOnce(c2, eps_clip, agent, target_score=270, target_success_rate=0.95, max_episodes=5000, max_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_old.state_dict(), f'PPO_LunarLander_{N}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'PPO_LunarLander_{N}.pth')\n",
    "model = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model.policy_old.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = EvaluateOnce(model, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the evaluation result and training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = excellent_result['total_rewards']\n",
    "final_rewards = excellent_result['final_rewards']\n",
    "moving_average_rewards = excellent_result['moving_average_rewards']\n",
    "test_total_rewards = eval_result['test_total_rewards']\n",
    "test_final_rewards = eval_result['test_final_rewards']\n",
    "success_rate = eval_result['success_rate']\n",
    "data = {'total_rewards': total_rewards, 'final_rewards': final_rewards, 'moving_average_rewards': moving_average_rewards, 'test_total_rewards': test_total_rewards, 'test_final_rewards': test_final_rewards, 'success_rate': success_rate}\n",
    "\n",
    "with open(f'PPO_Result_{N}.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Saving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir demo_gifs/trial{N}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_demo(agent, num=10, mode = 'trail'):\n",
    "    agent.eval()\n",
    "    fig = plt.figure()\n",
    "    total_rewards = []\n",
    "\n",
    "    for i in range(num):\n",
    "        ims = []\n",
    "        agent.eval()\n",
    "        actions = []\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done :\n",
    "            step +=1\n",
    "            action = model.act(state)\n",
    "            actions.append(action)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            im = plt.imshow(env.render(), animated=True)\n",
    "            ims.append([im])\n",
    "        total_rewards.append(total_reward)\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=25, blit=True, repeat_delay=1000)\n",
    "        if mode=='trail':\n",
    "            ani.save(f'./demo_gifs/trial{N}/demo_{N}_{i}.gif')\n",
    "        if mode=='reward shaping':\n",
    "            ani.save(f'./demo_gifs/reward_shaping{N}/reward_shaping_{N}_{i}.gif')\n",
    "        if mode=='compare':\n",
    "            ani.save(f'./demo_gifs/reward_shaping{N}/compare_{N}_{i}.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 10 demos for model loaded just now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_demo(model, 10, 'trail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir demo_gifs/reward_shaping{N}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix(env, 234)\n",
    "r_s_agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "training_result_rs = TrainOnce(c2, eps_clip, r_s_agent, reward_mode=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_result_rs.pkl', 'wb') as f:\n",
    "    pickle.dump(training_result_rs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to normal process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "training_result_normal = TrainOnce(c2, eps_clip, normal_agent, reward_mode=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_result_normal.pkl', 'wb') as f:\n",
    "    pickle.dump(training_result_normal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.title(\"Reward Shaping Helps to Learn\", fontsize=20)\n",
    "moving_average_rewards1 = training_result_normal['moving_average_rewards']\n",
    "plt.plot(moving_average_rewards1, color=(147/255,103/255,153/255), label='Normal')\n",
    "moving_average_rewards2 = training_result_rs['moving_average_rewards']\n",
    "plt.plot(moving_average_rewards2, color=\"green\", label='Reward Shaping')\n",
    "plt.xlabel('Episode Needed')\n",
    "plt.ylabel('Average Rewards')\n",
    "plt.axhline(y=200, color='r', linestyle='--')\n",
    "\n",
    "x1 = len(moving_average_rewards1)\n",
    "x2 = len(moving_average_rewards2)\n",
    "plt.plot([x1,x1], [-400,200], color = \"grey\", linestyle='--')\n",
    "plt.plot([x2,x2], [-400,200], color = \"grey\", linestyle='--')\n",
    "plt.text(x1+10,0, f'{x1} episodes')\n",
    "plt.text(x2+10,0, f'{x2} episodes')\n",
    "\n",
    "plt.ylim(-400, 400)\n",
    "plt.xlim(0, 1300)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
