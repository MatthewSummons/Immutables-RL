{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543\n",
    "def fix(env, seed):\n",
    "    env.action_space.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "env = gym.make('LunarLander-v2' ,render_mode='rgb_array')\n",
    "fix(env, seed) # fix the environment Do not revise this !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00184774,  1.4150664 , -0.18716355,  0.18427911,  0.00214778,\n",
       "         0.0423953 ,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you don't want to overwrite your previous results, increment the N by 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "N = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Memory\n",
    "\n",
    "store the actions, states, rewards, is_termminal, and log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: ActorCritic\n",
    "\n",
    "### 2 fully connected networks\n",
    "1. actor/action layer: input the states, follow the policy and yields an action\n",
    "2. critic/value layer: input the states, evaluate the current estimated rewards\n",
    "\n",
    "### act function\n",
    "\n",
    "Generate a probability distribution of the actions using action layer taking state as the input, sample one action and store these information in the memory.\n",
    "\n",
    "### evaluate function\n",
    "\n",
    "Take a state as input and generate action probability distribution, calculate the entropy of this distribution and evaluate the log probability of the given action. Use the critic to evaluate the expected rewards under current state. Return all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, n_latent_var),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "\n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "               nn.Linear(state_dim, 128),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(128, n_latent_var),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(n_latent_var, 1)\n",
    "               )\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO agent\n",
    "\n",
    "Some explaination of concepts and functions:\n",
    "\n",
    "### gamma\n",
    "\n",
    "If close to 1, future rewards will be considered more\n",
    "If close to 0, current rewards wiil take more proportion\n",
    "\n",
    "### state_values\n",
    "\n",
    "The output of the critic, which evaluates the state and give an expected future reward\n",
    "\n",
    "### rewards\n",
    "\n",
    "The static list of discounted rewards calculated by the data in old trajectory (when adopting old policy)\n",
    "\n",
    "### advantages = rewards - state_values.detach()\n",
    "\n",
    "How better the old policy perform than current policy. if positive, actual reward is higher than state value, it is better to stick to current action, vice versa\n",
    "\n",
    "### update()\n",
    "\n",
    "Calling this function will update the networks for K epochs.\n",
    "\n",
    "### step()\n",
    "\n",
    "Record the information of one step the agent just took in the environment. Every 1000(update_timestep) steps reached will trigger update(). Therefore, the update frequency is guaranteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var,update_timestep, lr, betas, gamma, K_epochs, eps_clip, c1, c2):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.update_timestep = update_timestep\n",
    "        self.timestep = 0\n",
    "        self.memory = Memory()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.update_cnt = 0\n",
    "\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "\n",
    "    def update(self):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(self.memory.states).detach().to(device)\n",
    "        old_actions = torch.stack(self.memory.actions).detach().to(device)\n",
    "        old_logprobs = torch.stack(self.memory.logprobs).detach().to(device)\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values \n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss \n",
    "            advantages = rewards - state_values.detach()\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages # clip to avoid large difference between samples\n",
    "            loss = -torch.min(surr1, surr2)  + self.c1*self.MseLoss(state_values, rewards) + self.c2*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    def step(self, reward, done):\n",
    "        self.timestep += 1 \n",
    "        # Saving reward and is_terminal:\n",
    "        self.memory.rewards.append(reward)\n",
    "        self.memory.is_terminals.append(done)\n",
    "\n",
    "        # update the policy per \"update_timestep\"\n",
    "        if self.timestep % self.update_timestep == 0:\n",
    "            self.update()\n",
    "            self.memory.clear_memory()\n",
    "            self.timestep = 0\n",
    "            self.update_cnt += 1\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.policy_old.act(state, self.memory)\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "        self.policy_old.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy.eval()\n",
    "        self.policy_old.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(total_rewards):\n",
    "    if len(total_rewards) == 0:\n",
    "        return 0\n",
    "    if len(total_rewards) < 99:\n",
    "        return np.mean(total_rewards)\n",
    "    else:\n",
    "        return np.mean(total_rewards[-100:])\n",
    "    \n",
    "def reward_shaping(pre_state, state, type):\n",
    "    if (type == \"center\"):\n",
    "        pre_shaping = -20*np.abs(pre_state[0])\n",
    "        shaping = -20*np.abs(state[0])\n",
    "    if (type == \"horizontal\"):\n",
    "        pre_shaping = -1*np.abs(pre_state[4])\n",
    "        shaping = -1*np.abs(state[4])\n",
    "    if (type == \"both\"):\n",
    "        pre_shaping = -10*np.abs(pre_state[0])-5*abs(pre_state[4])\n",
    "        shaping = -10*np.abs(state[0])-5*abs(state[4])\n",
    "    if (type == \"none\"):\n",
    "        pre_shaping, shaping = 0, 0\n",
    "    return shaping - pre_shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It will take the agent as input and train it until some standard is satisfied. Then return the some results data as a disctionary.\n",
    "\n",
    "### You can set some parameters when calling this training function:\n",
    "1. reward_mode\n",
    "   \n",
    "    When doing experiment with reshape the reward function, this could be set to \"center\", \"horizontal\" or \"both\". When training normally, set this to \"none\".\n",
    "3. target_score\n",
    "   \n",
    "    If moving average score in training over previous 100 episodes reaches this score, the training will be terminated.\n",
    "4. target_success_rate\n",
    "   \n",
    "    If the landing success rate over previous 300 episodes reaches this value, the training will be terminated.\n",
    "6. max_episodes\n",
    "   \n",
    "    You cannot train more than this number of episodes.\n",
    "8. max_steps\n",
    "   \n",
    "    Use for debug.\n",
    "\n",
    "### Return value:\n",
    "1. total_rewards\n",
    "\n",
    "    The list of rewards that the lander was rewarded in every episode during the training process.\n",
    "2. moving_average_rewards\n",
    "\n",
    "    Use a sliding window of size 100 to show the average rewards the agent got through training.\n",
    "3. final_rewards\n",
    "\n",
    "    The list of the final rewards in each episode, which indicates whether the lander had successfully landed. If the final reward is 100, it lands successfully; if -100, it crashes in the end.\n",
    "\n",
    "5. num_episodes\n",
    "\n",
    "    The number of episodes.\n",
    "6. success_rate\n",
    "\n",
    "    This is the success landing rate of last 300 episodes. So, it changes across the training process. In the end, it is returned as the success rate of last 300 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainOnce(c2, eps_clip, agent, reward_mode = \"none\", target_score = 200, target_success_rate = 0.9, max_episodes=2000, max_steps=900):\n",
    "    agent.train()\n",
    "    print(f\"Training with c2: {c2}, eps_clip: {eps_clip}\")\n",
    "    total_rewards = []\n",
    "    final_rewards = []\n",
    "    moving_average_rewards = []\n",
    "    num_episodes = 0\n",
    "    success_rate = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        total_shaped_reward = 0\n",
    "        step = 0\n",
    "        # update_cnt = agent.update_cnt\n",
    "        while True:\n",
    "            step +=1\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            shaped_reward = reward + reward_shaping(state, next_state, reward_mode)\n",
    "            total_shaped_reward += shaped_reward # use the shaped reward to train\n",
    "            agent.step(shaped_reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            # if done or step >= max_steps:\n",
    "            if done:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "        moving_average_rewards.append(moving_average(total_rewards))\n",
    "        success_rate = final_rewards[-300:].count(100) / 300\n",
    "\n",
    "        if (num_episodes % 100 == 0):\n",
    "            print(f\"Update Count: {agent.update_cnt}, average rewards: {moving_average_rewards[-1]}, episode: {num_episodes}, success rate: {success_rate}\")\n",
    "        \n",
    "        # finish the training if any one condition is reached\n",
    "        if (moving_average(total_rewards) >= target_score):\n",
    "            print(f\"Target score reached in {num_episodes} episodes!\")\n",
    "            break\n",
    "        if (success_rate >= target_success_rate):\n",
    "            print(f\"Target success rate reached in {num_episodes} episodes!\")\n",
    "            break\n",
    "        if (num_episodes >= max_episodes):\n",
    "            print(f\"Max episodes {max_episodes} reached!\")\n",
    "            break\n",
    "\n",
    "    training_result = {\n",
    "        \"total_rewards\": total_rewards,\n",
    "        \"moving_average_rewards\": moving_average_rewards,\n",
    "        \"final_rewards\": final_rewards,\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    return training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function\n",
    "\n",
    "Switch the model to evaluate mode and test for ``eval_num`` (500 by defaults) rounds of games, and calculate the related data. Return a dictionary including related information, where the definition is similar to those in Training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateOnce(agent, eval_num = 500):\n",
    "    agent.eval()\n",
    "    test_total_rewards = []\n",
    "    test_final_rewards = []\n",
    "    success_rate = 0\n",
    "    prg_bar = tqdm(range(eval_num))\n",
    "    for i in prg_bar:\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "            if (step>3000):\n",
    "                done = True\n",
    "        test_total_rewards.append(total_reward)\n",
    "        test_final_rewards.append(reward)\n",
    "    success_rate = test_final_rewards.count(100) / eval_num\n",
    "    print(f\"Test success rate {success_rate} over {eval_num} games\")\n",
    "    eval_result = {\n",
    "        \"test_total_rewards\": test_total_rewards,\n",
    "        \"test_final_rewards\": test_final_rewards,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BayesianOptimization to find the best pair of c2 and eps_clip\n",
    "\n",
    "Modify the \"if\" condition to enable or disable the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 8 \n",
    "action_dim = 4 \n",
    "n_latent_var = 64\n",
    "update_timestep = 1000\n",
    "c1 = 0.5\n",
    "lr = 0.003\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99\n",
    "K_epochs = 4\n",
    "\n",
    "\n",
    "c2=-0.008\n",
    "eps_clip=0.2\n",
    "if (False):\n",
    "    p_bounds = {'c2': (-0.01, -0.005), 'eps_clip': (0.1,0.3)}\n",
    "\n",
    "    def PPO_fn(c2, eps_clip):\n",
    "        agent = PPOAgent(state_dim,action_dim,n_latent_var,update_timestep,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "        training_result = TrainOnce(c2, eps_clip, agent, target_score=200)\n",
    "        return -training_result[\"num_episodes\"]\n",
    "    HyperParamOptimizer = BayesianOptimization(\n",
    "        f=PPO_fn,\n",
    "        pbounds=p_bounds,\n",
    "        random_state=1\n",
    "    )\n",
    "    HyperParamOptimizer.probe(\n",
    "        params=[-0.008, 0.20],\n",
    "        lazy=True\n",
    "    )\n",
    "    HyperParamOptimizer.maximize(\n",
    "        init_points=2,\n",
    "        n_iter=7\n",
    "    )\n",
    "    print(HyperParamOptimizer.max)\n",
    "    c2 = HyperParamOptimizer.max['params']['c2']\n",
    "    eps_clip = HyperParamOptimizer.max['params']['eps_clip']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a excellent model\n",
    "\n",
    "## Set some requirements, and training will stop whenever one of them is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with c2: -0.008, eps_clip: 0.2\n",
      "Update Count: 10, average rewards: -127.6402965183344, episode: 100, success rate: 0.0\n",
      "Update Count: 56, average rewards: -89.21321211910086, episode: 200, success rate: 0.0033333333333333335\n",
      "Update Count: 65, average rewards: -58.725042997353086, episode: 300, success rate: 0.0033333333333333335\n",
      "Update Count: 79, average rewards: 2.9637322092168636, episode: 400, success rate: 0.016666666666666666\n",
      "Update Count: 108, average rewards: 128.18420242753137, episode: 500, success rate: 0.2\n",
      "Update Count: 136, average rewards: 173.05389378341843, episode: 600, success rate: 0.43333333333333335\n",
      "Update Count: 160, average rewards: 196.76495749152167, episode: 700, success rate: 0.6666666666666666\n",
      "Update Count: 188, average rewards: 171.46960568824994, episode: 800, success rate: 0.74\n",
      "Update Count: 224, average rewards: 195.5249290532181, episode: 900, success rate: 0.7833333333333333\n",
      "Update Count: 257, average rewards: 222.22196449662712, episode: 1000, success rate: 0.81\n",
      "Update Count: 284, average rewards: 190.5318002045195, episode: 1100, success rate: 0.78\n",
      "Update Count: 315, average rewards: 176.27407182321699, episode: 1200, success rate: 0.7266666666666667\n",
      "Update Count: 350, average rewards: 206.50172307681348, episode: 1300, success rate: 0.7333333333333333\n",
      "Update Count: 381, average rewards: 231.6376670202549, episode: 1400, success rate: 0.8033333333333333\n",
      "Update Count: 411, average rewards: 213.4228357687449, episode: 1500, success rate: 0.8566666666666667\n",
      "Update Count: 436, average rewards: 227.05382361797737, episode: 1600, success rate: 0.86\n",
      "Update Count: 458, average rewards: 254.37705858656898, episode: 1700, success rate: 0.87\n",
      "Update Count: 480, average rewards: 240.62952896454289, episode: 1800, success rate: 0.8766666666666667\n",
      "Update Count: 500, average rewards: 233.80329398444871, episode: 1900, success rate: 0.87\n",
      "Update Count: 520, average rewards: 216.52138865005543, episode: 2000, success rate: 0.81\n",
      "Update Count: 541, average rewards: 222.8978452988755, episode: 2100, success rate: 0.8\n",
      "Update Count: 562, average rewards: 246.31102552966712, episode: 2200, success rate: 0.8266666666666667\n",
      "Update Count: 583, average rewards: 240.8486615072443, episode: 2300, success rate: 0.8733333333333333\n",
      "Update Count: 636, average rewards: 170.69320888411792, episode: 2400, success rate: 0.8666666666666667\n",
      "Update Count: 656, average rewards: 209.8308277146725, episode: 2500, success rate: 0.8166666666666667\n",
      "Update Count: 680, average rewards: 208.37597452843806, episode: 2600, success rate: 0.79\n",
      "Update Count: 702, average rewards: 227.55630849610216, episode: 2700, success rate: 0.8133333333333334\n",
      "Update Count: 723, average rewards: 228.17013128185795, episode: 2800, success rate: 0.84\n",
      "Update Count: 746, average rewards: 241.77791784120936, episode: 2900, success rate: 0.8733333333333333\n",
      "Update Count: 776, average rewards: 194.41422996156257, episode: 3000, success rate: 0.8366666666666667\n",
      "Update Count: 800, average rewards: 245.31078008112434, episode: 3100, success rate: 0.8666666666666667\n",
      "Update Count: 898, average rewards: 152.1944391403888, episode: 3200, success rate: 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "excellent_result = TrainOnce(c2, eps_clip, agent, target_score=270, target_success_rate=0.95, max_episodes=5000, max_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_old.state_dict(), f'./trained_models/PPO_LunarLander_{N}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'./trained_models/PPO_LunarLander_{N}.pth')\n",
    "model = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep,lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "model.policy.load_state_dict(state_dict)\n",
    "model.policy_old.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate for 500 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = EvaluateOnce(model, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the evaluation result and training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = excellent_result['total_rewards']\n",
    "final_rewards = excellent_result['final_rewards']\n",
    "moving_average_rewards = excellent_result['moving_average_rewards']\n",
    "test_total_rewards = eval_result['test_total_rewards']\n",
    "test_final_rewards = eval_result['test_final_rewards']\n",
    "success_rate = eval_result['success_rate']\n",
    "data = {'total_rewards': total_rewards, 'final_rewards': final_rewards, 'moving_average_rewards': moving_average_rewards, 'test_total_rewards': test_total_rewards, 'test_final_rewards': test_final_rewards, 'success_rate': success_rate}\n",
    "\n",
    "with open(f'./results/PPO_Result_{N}.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Saving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir demo_gifs/trial{N}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_demo(agent, num=10, mode = 'trail'):\n",
    "    agent.eval()\n",
    "    fig = plt.figure()\n",
    "    total_rewards = []\n",
    "\n",
    "    for i in range(num):\n",
    "        ims = []\n",
    "        agent.eval()\n",
    "        actions = []\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done :\n",
    "            step +=1\n",
    "            action = model.act(state)\n",
    "            actions.append(action)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            im = plt.imshow(env.render(), animated=True)\n",
    "            ims.append([im])\n",
    "        total_rewards.append(total_reward)\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=25, blit=True, repeat_delay=1000)\n",
    "        if mode=='trail':\n",
    "            ani.save(f'./demo_gifs/trial{N}/demo_{N}_{i}.gif')\n",
    "        if mode=='reward shaping':\n",
    "            ani.save(f'./demo_gifs/trial{N}/reward_shaping_{N}_{i}.gif')\n",
    "        if mode=='compare':\n",
    "            ani.save(f'./demo_gifs/trial{N}/compare_{N}_{i}.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 10 demos for model loaded just now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_demo(model, 10, 'trail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir demo_gifs/reward_shaping{N}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix(env, 234)\n",
    "r_s_agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "training_result_rs = TrainOnce(c2, eps_clip, r_s_agent, reward_mode=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_result_rs.pkl', 'wb') as f:\n",
    "    pickle.dump(training_result_rs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to normal process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_agent = PPOAgent(state_dim ,action_dim,n_latent_var,update_timestep, lr,betas,gamma,K_epochs,eps_clip,c1,c2)\n",
    "training_result_normal = TrainOnce(c2, eps_clip, normal_agent, reward_mode=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_result_normal.pkl', 'wb') as f:\n",
    "    pickle.dump(training_result_normal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.title(\"Reward Shaping Helps to Learn\", fontsize=20)\n",
    "moving_average_rewards1 = training_result_normal['moving_average_rewards']\n",
    "plt.plot(moving_average_rewards1, color=(147/255,103/255,153/255), label='Normal')\n",
    "moving_average_rewards2 = training_result_rs['moving_average_rewards']\n",
    "plt.plot(moving_average_rewards2, color=\"green\", label='Reward Shaping')\n",
    "plt.xlabel('Episode Needed')\n",
    "plt.ylabel('Average Rewards')\n",
    "plt.axhline(y=200, color='r', linestyle='--')\n",
    "\n",
    "x1 = len(moving_average_rewards1)\n",
    "x2 = len(moving_average_rewards2)\n",
    "plt.plot([x1,x1], [-400,200], color = \"grey\", linestyle='--')\n",
    "plt.plot([x2,x2], [-400,200], color = \"grey\", linestyle='--')\n",
    "plt.text(x1+10,0, f'{x1} episodes')\n",
    "plt.text(x2+10,0, f'{x2} episodes')\n",
    "\n",
    "plt.ylim(-400, 400)\n",
    "plt.xlim(0, 1300)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
